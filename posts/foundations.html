<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<link rel="stylesheet" href="styles.css">

</head>

<body lang=EN-US link="#467886" vlink="#96607D" style='word-wrap:break-word'>

<div class=WordSection1>

<p class=MsoNormal>Large language models have historically been considered a
subset of natural language processing. However, in the past half-decade, we
have witnessed unprecedented advances in these models. This progress raises the
question of whether categorizing LLMs solely within the scope of NLP might be
unnecessarily restrictive. The following blog post presents my learnings,
insights, and thoughts from reading multiple papers, blog posts and various
other pieces of text into building and theoretical understanding of language
models. </p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b><span style='font-size:14.0pt;line-height:116%'>1. Tokenization</span></b></p>

<p class=MsoNormal>Tokenization is the process where a piece of text is broken
down into smaller units called tokens. These tokens can be words, sub-words,
characters, or even smaller units, depending on the tokenization strategy employed.
</p>

<p class=MsoNormal>LLMs are predictive models. They are trained to predict the
next token in a sequence, given all the other tokens that appear in the
sequence. To contextualize this a bit, <a name="_Int_Mf5r63T2">let’s</a>
consider a simple sequence of words, with the final word missing:</p>

<p class=MsoNormal align=center style='text-align:center'>The quick brown fox
jumped over the lazy ___</p>

<p class=MsoNormal>The task of the LLM here is to predict the missing word in
the sequence. The LLM starts by first breaking the sentence in individual words
like this, </p>

<p class=MsoNormal align=center style='text-align:center'>['The', 'quick',
'brown', 'fox', 'jumped', 'over', 'the', 'lazy']</p>

<p class=MsoNormal>Now each word is assigned an ID. Now the vector looks
something like this: </p>

<p class=MsoNormal align=center style='text-align:center'>[1996, 4248, 2829,
4419, 5598, 2058, 1996, 13971]</p>

<p class=MsoNormal><span style='color:black'>These IDs map to entries in a
token embedding table (which will be discussed in the next chapter). In this
example, we have chosen words as our tokens. Alternatively, we could use
characters as tokens, which would result in the following vector:</span> </p>

<p class=MsoNormal align=center style='text-align:center'>['T', 'h', 'e', ' ',
'q', 'u', 'i', 'c', 'k', ...]</p>

<p class=MsoNormal>Such a language model would be <a
href="https://towardsdatascience.com/character-level-language-model-1439f5dd87fe"><i>character-based</i></a>,
as opposed to <i>word-based</i>. As opposed to these techniques, modern LLMs
have adopted sub-word tokenization. </p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>1.1 Byte-Pair Encoding</b></p>

<p class=MsoNormal style='margin-bottom:12.0pt'><a
href="https://en.wikipedia.org/wiki/Byte_pair_encoding">Byte-Pair Encoding</a><span
style='color:black'> is a sub-word tokenization algorithm used in natural
language processing (NLP) to efficiently break text into smaller units
(tokens). It balances character-based and word-based tokenization, allowing
models to handle rare words and out-of-vocabulary terms effectively.  Let us
assume </span>the data to be encoded is</p>

<p class=MsoNormal align=center style='margin-bottom:12.0pt;text-align:center'>aaabdaaabac</p>

<p class=MsoNormal style='margin-bottom:12.0pt'>The byte pair &quot;<a
name="_Int_ZOwTO60a">aa</a>&quot; occurs most often, so it will be replaced by
a byte that is not used in the data, such as &quot;Z&quot;. Now there is the
following data and replacement table:</p>

<p class=MsoNormal align=center style='margin-bottom:12.0pt;text-align:center'>ZabdZabac<br>
Z=aa</p>

<p class=MsoNormal style='margin-bottom:12.0pt'>Then the process is repeated
with byte pair &quot;ab&quot;, replacing it with &quot;Y&quot;: </p>

<p class=MsoNormal align=center style='margin-bottom:12.0pt;text-align:center'>ZYdZYac<br>
Y=ab<br>
Z=aa</p>

<p class=MsoNormal style='margin-bottom:12.0pt'>To visualize this, I encourage
you to play with a <a href="https://tiktokenizer.vercel.app/">tokenizer</a> .</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b><span style='font-size:14.0pt;line-height:116%'>2.
Embedding</span></b></p>

<p class=MsoNormal>A word embedding table <span style='color:black'>converts
discrete words (or tokens) into continuous numerical vectors. These vectors
capture semantic and syntactic relationships between words, allowing LLMs to
process language meaningfully. It is essentially a lookup table that maps each
token (from a tokenizer like BPE) to a high-dimensional vector.  For example,
the vector for the word cat can look something like this: </span></p>

<p class=MsoNormal align=center style='text-align:center'>[0.0074, 0.0030,
-0.0105, 0.0742, 0.0765, -0.0011, 0.0265, 0.0106, 0.0191, 0.0038, -0.0468,
-0.0212, 0.0091, 0.0030, -0.0563, -0.0396, -0.0998, -0.0796, …, 0.0002]</p>

<p class=MsoNormal>To understand the embedding table better, let's take a look
at an example.</p>

<p class=MsoNormal align=center style='text-align:center'> <img border=0
width=574 height=341 id="Picture 1336054260"
src="Foundations%20of%20Large%20Language%20Models_files/image001.jpg"
alt="The Ultimate Guide to Word Embeddings"></p>

<p class=MsoNormal>The following is a 2-dimensional lookup table. You may
notice similar words like sink, bathtub, kitchen etc being grouped together.
The relative distances<span style='color:black'> between words reflect how
often they appear in similar contexts in a dataset. Words that appear in
similar contexts are closer together in the embedding space. The color coding
represents different semantic categories to make visualization clearer. </span></p>

<p class=MsoNormal style='margin-bottom:12.0pt'>When a word has two unrelated
meanings, as with bank, linguists call them homonyms. When a word has two
closely related meanings, as with magazine, linguists call it polysemy. LLMs
are able to represent the same word with different vectors depending on the
context in which that word appears. </p>

<p class=MsoNormal><b><span style='font-size:14.0pt;line-height:116%'>&nbsp;</span></b></p>

<p class=MsoNormal><b><span style='font-size:14.0pt;line-height:116%'>3.
Pre-training</span></b></p>

<p class=MsoNormal>Pre-training has been widely popular since the early days of
NLP research. For example, early attempts to pre-train deep learning systems
include unsupervised learning for RNNs, deep feedforward neural networks, and
others. Models like BERT [<a href="https://arxiv.org/abs/1810.04805">Devlin et
al., 2019</a>]  and GPT were early breakthroughs in the field. </p>

<p class=MsoNormal> </p>

<p class=MsoNormal><b>3.1 Unsupervised, Supervised and Self-supervised
Pre-training. </b></p>

<p class=MsoNormal>Unsupervised learning represented one of the early
approaches to pre-training. Instead of optimizing for a pre-defined task,
models capture a generalized understanding. </p>

<p class=MsoNormal>Supervised pre-training, particularly in sequence models,
involves a two-step process: first, encoding input sequences into vector
representations, followed by applying a classification layer to create a
comprehensive classification system. During pre-training, the model learns to
map input sequences to outputs based on labeled data supervision. Subsequently,
the core sequence model (encoder) can be repurposed as a component within a new
model for different tasks. However, this approach faces limitations as model
complexity increases, primarily due to the growing requirement for labeled
training data. </p>

<p class=MsoNormal>The third approach is self-supervised learning. In this
approach, a neural network is trained using supervised signals generated by itself,
rather than those provided by humans. This is done by constructing its own
training tasks directly from unlabeled data, such as having pseudo labels.
Self-supervised learning has proven to be so successful that most current SOTA
(State-of-the-art) models are based on this paradigm. </p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><img border=0 width=623 height=282 id="Picture 304333866"
src="Foundations%20of%20Large%20Language%20Models_files/image002.png"></p>

<p class=MsoNormal>Additional Information: </p>

<p class=MsoNormal style='line-height:normal'>Zero-Shot learning (ZSL)</p>

<p class=MsoNormal>Zero-Shot Learning (ZSL) represents an advanced machine
learning paradigm where models demonstrate the capability to recognize and
execute tasks without prior exposure during their training phase. This
methodology enables models to generalize their knowledge to novel classes or
tasks without requiring specific training examples. The underlying principle
involves leveraging knowledge acquired from related tasks or classes to make
informed inferences about previously unseen scenarios. </p>

<p class=MsoNormal>Few-shot Learning (FSL)</p>

<p class=MsoNormal>Few-Shot Learning (FSL) describes a learning approach where
models develop competency using minimal training examples, typically ranging
from one to five instances. This methodology focuses on rapid adaptation and
generalization to new tasks with limited exposure to training data. However, a
significant limitation of FSL lies in its susceptibility to overfitting, as
models may become overly specialized to the small set of training examples.</p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>3.2 Adapting Pre-training Models </b></p>

<p class=MsoNormal><b>Sequence Encoding Models. </b></p>

<p class=MsoNormal><span style='color:black'>A sequence encoding model
transforms an input sequence of words or tokens into either a fixed-length
real-valued vector or a sequence of vectors, creating a mathematical
representation of the sequence. These models typically serve as components
within larger machine learning systems rather than functioning as independent
applications.</span></p>

<p class=MsoNormal><b>Sequence Generation Models</b>. </p>

<p class=MsoNormal><span style='color:black'>A sequence generation model
produces an output sequence of tokens conditional on a given input context.
These models utilize probabilistic methods to generate coherent sequences of
text, code, or other token-based content.</span></p>

<p class=MsoNormal><span style='color:black'>&nbsp;</span></p>

<p class=MsoNormal><b>3.3 Decoder-only Pre-training</b></p>

<p class=MsoNormal style='margin-bottom:12.0pt'>Decoder-only architecture
models focus solely on generating sequences [<a
href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">OpenAI
</a>]. They predict the next token based on the context provided by all the
previous tokens, making them autoregressive. The below image describes the
architecture of a decoder-only model. </p>

<p class=MsoNormal align=center style='text-align:center'><img border=0
width=216 height=414 id="Picture 17115068"
src="Foundations%20of%20Large%20Language%20Models_files/image003.png"></p>

<p class=MsoNormal><b>&nbsp;</b></p>

<p class=MsoNormal>Let us define a log-scale cross-entropy loss function L (p<sup><span
style='font-size:12.5pt;line-height:116%'>θ</span></sup><sub><span
style='font-size:12.5pt;line-height:116%'>i+1</span></sub><span
style='font-size:12.5pt;line-height:116%'>, p<sup>gold</sup><sub>i+1</sub>) to
measure the difference between the model prediction and the true prediction. Given
a sequence of ‘m’ tokens {x0, x1, ....xm}, the sum of the loss over the
positions {0, <a name="_Int_jWyHDe3K">1,...</a>., m-1}, is given by </span></p>

<p class=MsoNormal align=center style='text-align:center'><img border=0
width=624 height=147 id="Picture 1871023720"
src="Foundations%20of%20Large%20Language%20Models_files/image004.png"></p>

<p class=MsoNormal><span style='font-size:12.5pt;line-height:116%'>p<sup>gold</sup><sub>i+1
</sub>is the one-hot representation of x<sub>i+1</sub>. </span>This loss
function can be extended to a set of sequences ‘D.’ In this case, the objective
of pre-training is to find the best parameters that minimize the loss on ‘D’</p>

<p class=MsoNormal align=center style='text-align:center'><img border=0
width=555 height=106 id="Picture 1317118562"
src="Foundations%20of%20Large%20Language%20Models_files/image005.png"></p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal><b>3.4 Masked Language Modelling</b></p>

<p class=MsoNormal>Masked Language Modelling (MLM) helps to develop an understanding
of context and meaning by predicting masked(hidden) tokens in a sentence. Eg: </p>

<p class=MsoNormal style='margin-bottom:12.0pt'><i>Original</i>: “Deep learning
is transforming AI.”</p>

<p class=MsoNormal style='margin-top:12.0pt;margin-right:0in;margin-bottom:
12.0pt;margin-left:0in'><i>Masked</i>: “Deep learning is [MASK] AI.”</p>

<p class=MsoNormal>MLMs help models learn from both preceding and succeeding
words. This helps build a richer understanding and deeper embeddings. </p>

<p class=MsoNormal>There are different ways to mask tokens. We can randomly
select token in a sentence to mask them or we can also select consecutive
tokens as shown in <a href="https://arxiv.org/abs/1910.10683">Raffel et al.,
2020</a> . </p>

<p class=MsoNormal>Another popular technique is Denoising Training. </p>

<p class=MsoNormal><b>&nbsp;</b></p>

<p class=MsoNormal><b>&nbsp;</b></p>

<p class=MsoNormal><b>&nbsp;</b></p>

<p class=MsoNormal><b>&nbsp;</b></p>

<p class=MsoNormal><b>&nbsp;</b></p>

<p class=MsoNormal><b>&nbsp;</b></p>

<p class=MsoNormal><b>&nbsp;</b></p>

<p class=MsoNormal><b>&nbsp;</b></p>

<p class=MsoNormal><b>&nbsp;</b></p>

<p class=MsoNormal><b>&nbsp;</b></p>

<p class=MsoNormal><b>&nbsp;</b></p>

<p class=MsoNormal><b><span style='font-size:14.0pt;line-height:116%'>&nbsp;</span></b></p>

<p class=MsoNormal><b><span style='font-size:14.0pt;line-height:116%'>Additional
Resources</span></b></p>

<p class=MsoListParagraphCxSpFirst style='text-indent:-.25in'>1.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Foundations
of large language models: <a href="https://arxiv.org/abs/2501.09223">Tong et
al., 2025</a></p>

<p class=MsoListParagraphCxSpMiddle style='text-indent:-.25in'>2.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>Sequence
to Sequence Learning with Neural Networks: <a
href="https://arxiv.org/abs/1409.3215">Sutskever et al., 2014</a> </p>

<p class=MsoListParagraphCxSpLast style='text-indent:-.25in'>3.<span
style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>BERT:
<a href="https://arxiv.org/abs/1810.04805">Delvin et al., 2018</a> </p>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal style='line-height:normal'>&nbsp;</p>

</div>

</body>

</html>
